---
title: "p8105_hw5_xm2356"
author: "Xinyin Miao (xm2356)"
date: "2025-11-14"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(purrr)
set.seed(1)

knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE,
  fig.align = "center", fig.width = 7, fig.height = 4, dpi = 200
)

```

# Problem 1 

```{r}
# write function
bday_sim = function(n_room){
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
}

# the probability of at least one shared birthday
bday_sim_results = 
  expand_grid(
    bdays = 2:50,        
    iter = 1:10000        
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)   
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result)
  )

bday_sim_results

# plot
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Probability of at least one shared birthday",
    x = "Group size",
    y = "Estimated probability"
  ) +
  theme_minimal(base_size = 12)

```

The simulated results show the smooth S-shaped curve, showing a sharply increasing relationship between group size and the probability that at least two people share a birthday. When the group size is small (fewer than 10 people), the probability of a shared birthday is close to zero. As the number of people increases, this probability rises rapidly, reaching approximately 50% when there are about 23 individuals in the room. Beyond 40 people, the probability exceeds 90%, and by 50 it approaches certainty. These results highlight that: even with only a few dozen people, coincidences such as shared birthdays become surprisingly likely.


# Problem 2

```{r}
n <- 30
sigma <- 5
alpha <- 0.05
mus <- 0:6
n_sims <- 5000

sim_one_mu <- function(mu, n_sims, n, sigma, alpha = 0.05) {
  tibble(sim = 1:n_sims) |>
    mutate(
      x = map(sim, ~ rnorm(n, mean = mu, sd = sigma)),
      t_out = map(x, ~ t.test(.x, mu = 0, conf.level = 1 - alpha)),
      t_tidy = map(t_out, broom::tidy)
    ) |>
    select(-x, -t_out) |>
    unnest(t_tidy) |>
    transmute(mu = mu, estimate = estimate, p_value = p.value)
}

```

```{r}
p2_sims <- map_dfr(mus, ~ sim_one_mu(.x, n_sims, n, sigma, alpha))
```

## Power curve: proportion of rejections vs mu
```{r}
p2_power <- p2_sims |>
  group_by(mu) |>
  summarise(power = mean(p_value < alpha), .groups = "drop")

p_power <- p2_power |>
  ggplot(aes(mu, power)) +
  geom_line() +
  geom_point(size = 1.5) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Power of one-sample t-test (n = 30, œÉ = 5, Œ± = 0.05)",
    x = expression(true~mu),
    y = "Power (rejection rate)"
  ) +
  theme_minimal(base_size = 12)

p_power
```

The plot shows that statistical power increases rapidly as the true mean (Œº) moves further away from zero. When Œº = 0, the probability of rejecting the null hypothesis is approximately 5%, as expected for a test with Œ± = 0.05. As the effect size grows, the power rises sharply: around Œº = 2, the test detects a true effect more than half of the time, and by Œº ‚â• 4, power approaches 100%. This strong positive association indicates that larger deviations from the null hypothesis make it easier for the t-test to identify true effects. 

## Average estimate vs mu
```{r}
p2_est <- p2_sims |>
  group_by(mu) |>
  summarise(
    avg_est_all = mean(estimate),
    avg_est_reject = mean(estimate[p_value < alpha]),
    .groups = "drop"
  )

p_est_combined <- p2_est |>
  ggplot(aes(x = mu)) +
  # line for all tests
  geom_line(aes(y = avg_est_all, color = "All tests"), linewidth = 1) +
  geom_point(aes(y = avg_est_all, color = "All tests"), size = 1.5) +
  # line for rejected tests
  geom_line(aes(y = avg_est_reject, color = "Rejected only"), linewidth = 1) +
  geom_point(aes(y = avg_est_reject, color = "Rejected only"), size = 1.5) +
  # custom color legend
  scale_color_manual(
    values = c("All tests" = "steelblue", "Rejected only" = "firebrick")
  ) +
  labs(
    title = "Average estimate of ŒºÃÇ across all vs. rejected tests",
    x = expression(True~mu),
    y = "Average estimate of ŒºÃÇ",
    color = "Sample group"
  ) +
  theme_minimal(base_size = 12) 

p_est_combined

```

The sample average of ùúáÃÇ across tests for which the null hypothesis was rejected is not approximately equal to the true value of ùúá, especially when the true effect size is small. In the combined plot, the red line (‚ÄúRejected only‚Äù) is generally above the blue line (‚ÄúAll tests‚Äù) for small Œº, though they overlap when the effect is large. This pattern shows that significant results tend to overestimate the true mean because only samples with unusually large random deviations achieve p < 0.05 when Œº is near zero. As Œº increases, this bias diminishes and the two lines converge. The blue line remains close to the true Œº throughout, confirming that the one-sample t-test is unbiased overall without selection on significance.

# Problem 3

```{r}
homicides_raw = readr::read_csv("data/homicide-data.csv") |> 
    janitor::clean_names()
```

The `homicides_raw` dataset has `r nrow(homicides_raw)` observations of criminal homicides reported in 50 large U.S. cities. Each row represents one homicide case. The dataset includes 12 variables describing the victim‚Äôs demographics (last name, first name, age, sex, race), the location of the homicide (city, state, latitude, longitude), the case outcome (disposition) and the case reported date. The disposition variable indicates whether the case was ‚ÄúClosed by arrest,‚Äù ‚ÄúClosed without arrest,‚Äù or remains ‚ÄúOpen/No arrest.‚Äù 

```{r}
homicides = homicides_raw |>
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Open/No arrest", "Closed without arrest")
  )
homicides |> 
  slice_head(n = 6)|>
  knitr::kable(digits = 3, caption = "Preview of cleaned data (first 6 rows)")
```

## Summarize totals and unsolved per city
```{r}
city_totals <- homicides |>
  group_by(city_state) |>
  summarise(
    total = n(),
    unsolved = sum(unsolved, na.rm = TRUE),
    .groups = "drop"
  )

city_totals |> 
  arrange(desc(total)) |> 
  slice_head(n = 10)|>
  knitr::kable(
    digits  = 0,
    caption = "Top 10 cities by number of homicide cases"
  )
  
```

## Baltimore 

```{r}
baltimore_counts <- city_totals |> filter(city_state == "Baltimore, MD")

baltimore_tidy <- broom::tidy(prop.test(
  x = baltimore_counts$unsolved,
  n = baltimore_counts$total
)) |>
  mutate(city_state = "Baltimore, MD") |>
  select(city_state, estimate, conf.low, conf.high) 

baltimore_tidy|>
  knitr::kable(digits = 4, caption = "Baltimore: estimated unsolved proportion (95% CI)")
```

## All cities

```{r}
city_props <- city_totals |>
  mutate(
    test_obj = map2(unsolved, total, ~ prop.test(.x, .y)),
    test_tidy = map(test_obj, broom::tidy)
  ) |>
  select(city_state, test_tidy) |>
  unnest(test_tidy) |>
  select(city_state, estimate, conf.low, conf.high) |> 
  arrange(desc(estimate)) 

city_props|>
  arrange(desc(estimate)) |>
  knitr::kable(digits = 4, caption = "Estimated unsolved proportions by city")

```

## Plot

```{r, fig.width=7.5, fig.height=12, dpi=300}
plot_city_ci <- ggplot(
  data = city_props,
  aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  geom_point(size = 2) +
  coord_flip() +
  labs(
    title = "Estimated proportion of unsolved homicides by city (95% CI)",
    x = NULL,
    y = "Proportion unsolved"
  ) +
  theme_minimal(base_size = 12)

plot_city_ci

```












