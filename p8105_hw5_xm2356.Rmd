---
title: "p8105_hw5_xm2356"
author: "Xinyin Miao (xm2356)"
date: "2025-11-11"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
set.seed(1)

knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE,
  fig.align = "center", fig.width = 7, fig.height = 4, dpi = 200
)


if (!dir.exists("data")) dir.create("data")
if (!dir.exists("results")) dir.create("results")
```

# Problem 1 

```{r}
bday_sim = function(n_room){
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
}


bday_sim_results = 
  expand_grid(
    bdays = 2:50,        
    iter = 1:10000        
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)   
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result)
  )


bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Probability of at least one shared birthday",
    x = "Group size",
    y = "Estimated probability"
  ) +
  theme_minimal(base_size = 12)

```

The simulated results show a sharply increasing relationship between group size and the probability that at least two people share a birthday. When the group size is small (fewer than 10 people), the probability of a shared birthday is close to zero. As the number of people increases, this probability rises rapidly, reaching approximately 50% when there are about 23 individuals in the room. Beyond 40 people, the probability exceeds 90%, and by 50 it approaches certainty. These results highlight how counter-intuitive probability can be: even with only a few dozen people, coincidences such as shared birthdays become surprisingly likely. The smooth S-shaped curve confirms the robustness of the simulation and aligns closely with theoretical expectations.


# Problem 2

```{r}
n <- 30
sigma <- 5
alpha <- 0.05
mus <- 0:6
n_sims <- 5000

sim_one_mu <- function(mu, n_sims, n, sigma, alpha = 0.05) {
  tibble(sim = 1:n_sims) |>
    mutate(
      x = map(sim, ~ rnorm(n, mean = mu, sd = sigma)),
      t_out = map(x, ~ t.test(.x, mu = 0, conf.level = 1 - alpha)),
      t_tidy = map(t_out, broom::tidy)
    ) |>
    select(-x, -t_out) |>
    unnest(t_tidy) |>
    transmute(mu = mu, estimate = estimate, p_value = p.value)
}

```

```{r}
p2_sims <- map_dfr(mus, ~ sim_one_mu(.x, n_sims, n, sigma, alpha))
```

## Power curve: proportion of rejections vs mu
```{r}
p2_power <- p2_sims |>
  group_by(mu) |>
  summarise(power = mean(p_value < alpha), .groups = "drop")

p_power <- p2_power |>
  ggplot(aes(mu, power)) +
  geom_line() +
  geom_point(size = 1.5) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Power of one-sample t-test (n = 30, Ïƒ = 5, Î± = 0.05)",
    x = expression(true~mu),
    y = "Power (rejection rate)"
  ) +
  theme_minimal(base_size = 12)

p_power
```

The plot shows that statistical power increases rapidly as the true mean (Î¼) moves further away from zero. When Î¼ = 0, the probability of rejecting the null hypothesis is approximately 5%, as expected for a test with Î± = 0.05. As the effect size grows, the power rises sharply: around Î¼ = 2, the test detects a true effect more than half of the time, and by Î¼ â‰¥ 4, power approaches 100%. This strong positive association indicates that larger deviations from the null hypothesis make it easier for the t-test to identify true effects. 

## Average estimate vs mu
```{r}
p2_est <- p2_sims |>
  group_by(mu) |>
  summarise(
    avg_est_all = mean(estimate),
    avg_est_reject = mean(estimate[p_value < alpha]),
    .groups = "drop"
  )

p_est_all <- ggplot(p2_est, aes(mu, avg_est_all)) +
  geom_line() + geom_point(size = 1.5) +
  labs(title = "Average estimate across all tests", x = expression(true~mu), y = "Average estimate") +
  theme_minimal(base_size = 12)

p_est_rej <- ggplot(p2_est, aes(mu, avg_est_reject)) +
  geom_line() + geom_point(size = 1.5) +
  labs(title = "Average estimate among rejected tests only", x = expression(true~mu), y = "Average estimate") +
  theme_minimal(base_size = 12)

p_est_all + p_est_rej + plot_layout(ncol = 2)

```

The left plot shows that the average estimated mean (ðœ‡Ì‚) across all simulations closely matches the true value of ðœ‡ for every setting, forming an almost perfect 1:1 line. This confirms that the one-sample t-test is an unbiased estimator of the true mean when considering all samples, regardless of statistical significance.

In contrast, the right plot shows that when we restrict attention only to samples where the null hypothesis was rejected (p < 0.05), the average estimate of ðœ‡Ì‚ is systematically higher than the true value, especially when the true effect size is small. This bias arises from selection based on statistical significanceâ€”only unusually large sample means tend to yield small p-values when the true effect is weak. Consequently, the conditional average exaggerates the magnitude of the effect.





