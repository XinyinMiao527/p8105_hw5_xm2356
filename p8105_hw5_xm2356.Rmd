---
title: "p8105_hw5_xm2356"
author: "Xinyin Miao (xm2356)"
date: "2025-11-11"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(purrr)
set.seed(1)

knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE,
  fig.align = "center", fig.width = 7, fig.height = 4, dpi = 200
)


if (!dir.exists("data")) dir.create("data")
if (!dir.exists("results")) dir.create("results")
```

# Problem 1 

```{r}
bday_sim = function(n_room){
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
}


bday_sim_results = 
  expand_grid(
    bdays = 2:50,        
    iter = 1:10000        
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)   
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result)
  )


bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Probability of at least one shared birthday",
    x = "Group size",
    y = "Estimated probability"
  ) +
  theme_minimal(base_size = 12)

```

The simulated results show a sharply increasing relationship between group size and the probability that at least two people share a birthday. When the group size is small (fewer than 10 people), the probability of a shared birthday is close to zero. As the number of people increases, this probability rises rapidly, reaching approximately 50% when there are about 23 individuals in the room. Beyond 40 people, the probability exceeds 90%, and by 50 it approaches certainty. These results highlight how counter-intuitive probability can be: even with only a few dozen people, coincidences such as shared birthdays become surprisingly likely. The smooth S-shaped curve confirms the robustness of the simulation and aligns closely with theoretical expectations.


# Problem 2

```{r}
n <- 30
sigma <- 5
alpha <- 0.05
mus <- 0:6
n_sims <- 5000

sim_one_mu <- function(mu, n_sims, n, sigma, alpha = 0.05) {
  tibble(sim = 1:n_sims) |>
    mutate(
      x = map(sim, ~ rnorm(n, mean = mu, sd = sigma)),
      t_out = map(x, ~ t.test(.x, mu = 0, conf.level = 1 - alpha)),
      t_tidy = map(t_out, broom::tidy)
    ) |>
    select(-x, -t_out) |>
    unnest(t_tidy) |>
    transmute(mu = mu, estimate = estimate, p_value = p.value)
}

```

```{r}
p2_sims <- map_dfr(mus, ~ sim_one_mu(.x, n_sims, n, sigma, alpha))
```

## Power curve: proportion of rejections vs mu
```{r}
p2_power <- p2_sims |>
  group_by(mu) |>
  summarise(power = mean(p_value < alpha), .groups = "drop")

p_power <- p2_power |>
  ggplot(aes(mu, power)) +
  geom_line() +
  geom_point(size = 1.5) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Power of one-sample t-test (n = 30, Ïƒ = 5, Î± = 0.05)",
    x = expression(true~mu),
    y = "Power (rejection rate)"
  ) +
  theme_minimal(base_size = 12)

p_power
```

The plot shows that statistical power increases rapidly as the true mean (Î¼) moves further away from zero. When Î¼ = 0, the probability of rejecting the null hypothesis is approximately 5%, as expected for a test with Î± = 0.05. As the effect size grows, the power rises sharply: around Î¼ = 2, the test detects a true effect more than half of the time, and by Î¼ â‰¥ 4, power approaches 100%. This strong positive association indicates that larger deviations from the null hypothesis make it easier for the t-test to identify true effects. 

## Average estimate vs mu
```{r}
p2_est <- p2_sims |>
  group_by(mu) |>
  summarise(
    avg_est_all = mean(estimate),
    avg_est_reject = mean(estimate[p_value < alpha]),
    .groups = "drop"
  )

p_est_all <- ggplot(p2_est, aes(mu, avg_est_all)) +
  geom_line() + geom_point(size = 1.5) +
  labs(title = "Average estimate across all tests", x = expression(true~mu), y = "Average estimate") +
  theme_minimal(base_size = 12)

p_est_rej <- ggplot(p2_est, aes(mu, avg_est_reject)) +
  geom_line() + geom_point(size = 1.5) +
  labs(title = "Average estimate among rejected tests only", x = expression(true~mu), y = "Average estimate") +
  theme_minimal(base_size = 12)

p_est_all + p_est_rej + plot_layout(ncol = 2)

```

The sample average of ðœ‡Ì‚ across tests for which the null hypothesis was rejected is not approximately equal to the true value of ðœ‡, particularly when the true effect size is small.
This occurs because statistical significance depends on both the observed effect and random sampling variation.
When ðœ‡ is close to zero, only samples with unusually large random deviations from the null tend to produce p-values below 0.05.
As a result, the subset of â€œsignificantâ€ results is biased toward overestimates of the true mean, inflating the conditional average of ðœ‡Ì‚.
This selection biasâ€”often causes the right-hand plot to lie consistently above the 1:1 line.
In contrast, when averaging across all tests (regardless of significance), the estimates are unbiased and align closely with the true Î¼.

# Problem 3

```{r}
homicides_raw = homicides_raw <- readr::read_csv("data/homicide-data.csv")
```

The raw dataset, obtained from The Washington Post, contains information on individual homicide cases reported in 50 large U.S. cities. Each row represents one homicide case. The dataset includes variables describing the victimâ€™s demographics (last name, first name, age, sex, race), the location of the homicide (city, state, latitude, longitude), the case outcome (disposition) and the case reported date. The disposition variable indicates whether the case was â€œClosed by arrest,â€ â€œClosed without arrest,â€ or remains â€œOpen/No arrest.â€ 

```{r}
homicides_raw |> 
  janitor::clean_names()

homicides <- homicides_raw |>
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Open/No arrest", "Closed without arrest")
  )
homicides |> 
  slice_head(n = 6)
```

## Summarize totals and unsolved per city
```{r}
city_totals <- homicides |>
  group_by(city_state) |>
  summarise(
    total = n(),
    unsolved = sum(unsolved, na.rm = TRUE),
    .groups = "drop"
  )

city_totals |> 
  arrange(desc(total)) |> 
  slice_head(n = 10)
  
```

## Baltimore 

```{r}
baltimore_counts <- city_totals |> filter(city_state == "Baltimore, MD")

baltimore_tidy <- broom::tidy(prop.test(
  x = baltimore_counts$unsolved,
  n = baltimore_counts$total
)) |>
  mutate(city_state = "Baltimore, MD") |>
  select(city_state, estimate, conf.low, conf.high)

baltimore_tidy
```

## For all cities

```{r}
city_props <- city_totals |>
  mutate(
    test_obj = map2(unsolved, total, ~ prop.test(.x, .y)),
    test_tidy = map(test_obj, broom::tidy)
  ) |>
  select(city_state, test_tidy) |>
  unnest(test_tidy) |>
  select(city_state, estimate, conf.low, conf.high)

city_props
```


```{r, fig.width=7.5, fig.height=12, dpi=300}
plot_city_ci <- city_props |>
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  geom_point(size = 2) +
  coord_flip() +
  labs(
    title = "Estimated proportion of unsolved homicides by city (95% CI)",
    x = NULL,
    y = "Proportion unsolved"
  ) +
  theme_minimal(base_size = 12)

plot_city_ci

```












