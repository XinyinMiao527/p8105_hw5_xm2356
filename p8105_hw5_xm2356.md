p8105_hw5_xm2356
================
Xinyin Miao (xm2356)
2025-11-14

# Problem 1

``` r
# write function
bday_sim = function(n_room){
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
}

# the probability of at least one shared birthday
bday_sim_results = 
  expand_grid(
    bdays = 2:50,        
    iter = 1:10000        
  ) |> 
  mutate(
    result = map_lgl(bdays, bday_sim)   
  ) |> 
  group_by(
    bdays
  ) |> 
  summarize(
    prob_repeat = mean(result)
  )

bday_sim_results
```

    ## # A tibble: 49 √ó 2
    ##    bdays prob_repeat
    ##    <int>       <dbl>
    ##  1     2      0.0024
    ##  2     3      0.0085
    ##  3     4      0.0167
    ##  4     5      0.0267
    ##  5     6      0.0399
    ##  6     7      0.0521
    ##  7     8      0.0751
    ##  8     9      0.0925
    ##  9    10      0.116 
    ## 10    11      0.139 
    ## # ‚Ñπ 39 more rows

``` r
# plot
bday_sim_results |> 
  ggplot(aes(x = bdays, y = prob_repeat)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Probability of at least one shared birthday",
    x = "Group size",
    y = "Estimated probability"
  ) +
  theme_minimal(base_size = 12)
```

<img src="p8105_hw5_xm2356_files/figure-gfm/unnamed-chunk-1-1.png" style="display: block; margin: auto;" />

The simulated results show the smooth S-shaped curve, showing a sharply
increasing relationship between group size and the probability that at
least two people share a birthday. When the group size is small (fewer
than 10 people), the probability of a shared birthday is close to zero.
As the number of people increases, this probability rises rapidly,
reaching approximately 50% when there are about 23 individuals in the
room. Beyond 40 people, the probability exceeds 90%, and by 50 it
approaches certainty. These results highlight that: even with only a few
dozen people, coincidences such as shared birthdays become surprisingly
likely.

# Problem 2

``` r
n <- 30
sigma <- 5
alpha <- 0.05
mus <- 0:6
n_sims <- 5000

sim_one_mu <- function(mu, n_sims, n, sigma, alpha = 0.05) {
  tibble(sim = 1:n_sims) |>
    mutate(
      x = map(sim, ~ rnorm(n, mean = mu, sd = sigma)),
      t_out = map(x, ~ t.test(.x, mu = 0, conf.level = 1 - alpha)),
      t_tidy = map(t_out, broom::tidy)
    ) |>
    select(-x, -t_out) |>
    unnest(t_tidy) |>
    transmute(mu = mu, estimate = estimate, p_value = p.value)
}
```

``` r
p2_sims <- map_dfr(mus, ~ sim_one_mu(.x, n_sims, n, sigma, alpha))
```

## Power curve: proportion of rejections vs mu

``` r
p2_power <- p2_sims |>
  group_by(mu) |>
  summarise(power = mean(p_value < alpha), .groups = "drop")

p_power <- p2_power |>
  ggplot(aes(mu, power)) +
  geom_line() +
  geom_point(size = 1.5) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Power of one-sample t-test (n = 30, œÉ = 5, Œ± = 0.05)",
    x = expression(true~mu),
    y = "Power (rejection rate)"
  ) +
  theme_minimal(base_size = 12)

p_power
```

<img src="p8105_hw5_xm2356_files/figure-gfm/unnamed-chunk-4-1.png" style="display: block; margin: auto;" />

The plot shows that statistical power increases rapidly as the true mean
(Œº) moves further away from zero. When Œº = 0, the probability of
rejecting the null hypothesis is approximately 5%, as expected for a
test with Œ± = 0.05. As the effect size grows, the power rises sharply:
around Œº = 2, the test detects a true effect more than half of the time,
and by Œº ‚â• 4, power approaches 100%. This strong positive association
indicates that larger deviations from the null hypothesis make it easier
for the t-test to identify true effects.

## Average estimate vs mu

``` r
p2_est <- p2_sims |>
  group_by(mu) |>
  summarise(
    avg_est_all = mean(estimate),
    avg_est_reject = mean(estimate[p_value < alpha]),
    .groups = "drop"
  )

p_est_combined <- p2_est |>
  ggplot(aes(x = mu)) +
  # line for all tests
  geom_line(aes(y = avg_est_all, color = "All tests"), linewidth = 1) +
  geom_point(aes(y = avg_est_all, color = "All tests"), size = 1.5) +
  # line for rejected tests
  geom_line(aes(y = avg_est_reject, color = "Rejected only"), linewidth = 1) +
  geom_point(aes(y = avg_est_reject, color = "Rejected only"), size = 1.5) +
  # custom color legend
  scale_color_manual(
    values = c("All tests" = "steelblue", "Rejected only" = "firebrick")
  ) +
  labs(
    title = "Average estimate of ŒºÃÇ across all vs. rejected tests",
    x = expression(True~mu),
    y = "Average estimate of ŒºÃÇ",
    color = "Sample group"
  ) +
  theme_minimal(base_size = 12) 

p_est_combined
```

<img src="p8105_hw5_xm2356_files/figure-gfm/unnamed-chunk-5-1.png" style="display: block; margin: auto;" />

The sample average of ùúáÃÇ across tests for which the null hypothesis was
rejected is not approximately equal to the true value of ùúá, especially
when the true effect size is small. In the combined plot, the red line
(‚ÄúRejected only‚Äù) is generally above the blue line (‚ÄúAll tests‚Äù) for
small Œº, though they overlap when the effect is large. This pattern
shows that significant results tend to overestimate the true mean
because only samples with unusually large random deviations achieve p \<
0.05 when Œº is near zero. As Œº increases, this bias diminishes and the
two lines converge. The blue line remains close to the true Œº
throughout, confirming that the one-sample t-test is unbiased overall
without selection on significance.

# Problem 3

``` r
homicides_raw = readr::read_csv("data/homicide-data.csv") |> 
    janitor::clean_names()
```

The `homicides_raw` dataset has 52179 observations of criminal homicides
reported in 50 large U.S. cities. Each row represents one homicide case.
The dataset includes 12 variables describing the victim‚Äôs demographics
(last name, first name, age, sex, race), the location of the homicide
(city, state, latitude, longitude), the case outcome (disposition) and
the case reported date. The disposition variable indicates whether the
case was ‚ÄúClosed by arrest,‚Äù ‚ÄúClosed without arrest,‚Äù or remains
‚ÄúOpen/No arrest.‚Äù

``` r
homicides = homicides_raw |>
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Open/No arrest", "Closed without arrest")
  )
homicides |> 
  slice_head(n = 6)|>
  knitr::kable(digits = 3, caption = "Preview of cleaned data (first 6 rows)")
```

| uid | reported_date | victim_last | victim_first | victim_race | victim_age | victim_sex | city | state | lat | lon | disposition | city_state | unsolved |
|:---|---:|:---|:---|:---|:---|:---|:---|:---|---:|---:|:---|:---|:---|
| Alb-000001 | 20100504 | GARCIA | JUAN | Hispanic | 78 | Male | Albuquerque | NM | 35.096 | -106.539 | Closed without arrest | Albuquerque, NM | TRUE |
| Alb-000002 | 20100216 | MONTOYA | CAMERON | Hispanic | 17 | Male | Albuquerque | NM | 35.057 | -106.715 | Closed by arrest | Albuquerque, NM | FALSE |
| Alb-000003 | 20100601 | SATTERFIELD | VIVIANA | White | 15 | Female | Albuquerque | NM | 35.086 | -106.696 | Closed without arrest | Albuquerque, NM | TRUE |
| Alb-000004 | 20100101 | MENDIOLA | CARLOS | Hispanic | 32 | Male | Albuquerque | NM | 35.078 | -106.556 | Closed by arrest | Albuquerque, NM | FALSE |
| Alb-000005 | 20100102 | MULA | VIVIAN | White | 72 | Female | Albuquerque | NM | 35.130 | -106.581 | Closed without arrest | Albuquerque, NM | TRUE |
| Alb-000006 | 20100126 | BOOK | GERALDINE | White | 91 | Female | Albuquerque | NM | 35.151 | -106.538 | Open/No arrest | Albuquerque, NM | TRUE |

Preview of cleaned data (first 6 rows)

## Summarize totals and unsolved per city

``` r
city_totals <- homicides |>
  group_by(city_state) |>
  summarise(
    total = n(),
    unsolved = sum(unsolved, na.rm = TRUE),
    .groups = "drop"
  )

city_totals |> 
  arrange(desc(total)) |> 
  slice_head(n = 10)|>
  knitr::kable(
    digits  = 0,
    caption = "Top 10 cities by number of homicide cases"
  )
```

| city_state       | total | unsolved |
|:-----------------|------:|---------:|
| Chicago, IL      |  5535 |     4073 |
| Philadelphia, PA |  3037 |     1360 |
| Houston, TX      |  2942 |     1493 |
| Baltimore, MD    |  2827 |     1825 |
| Detroit, MI      |  2519 |     1482 |
| Los Angeles, CA  |  2257 |     1106 |
| St.¬†Louis, MO    |  1677 |      905 |
| Dallas, TX       |  1567 |      754 |
| Memphis, TN      |  1514 |      483 |
| New Orleans, LA  |  1434 |      930 |

Top 10 cities by number of homicide cases

## Baltimore

``` r
baltimore_counts <- city_totals |> filter(city_state == "Baltimore, MD")

baltimore_tidy <- broom::tidy(prop.test(
  x = baltimore_counts$unsolved,
  n = baltimore_counts$total
)) |>
  mutate(city_state = "Baltimore, MD") |>
  select(city_state, estimate, conf.low, conf.high) 

baltimore_tidy|>
  knitr::kable(digits = 4, caption = "Baltimore: estimated unsolved proportion (95% CI)")
```

| city_state    | estimate | conf.low | conf.high |
|:--------------|---------:|---------:|----------:|
| Baltimore, MD |   0.6456 |   0.6276 |    0.6632 |

Baltimore: estimated unsolved proportion (95% CI)

## All cities

``` r
city_props <- city_totals |>
  mutate(
    test_obj = map2(unsolved, total, ~ prop.test(.x, .y)),
    test_tidy = map(test_obj, broom::tidy)
  ) |>
  select(city_state, test_tidy) |>
  unnest(test_tidy) |>
  select(city_state, estimate, conf.low, conf.high) |> 
  arrange(desc(estimate)) 

city_props|>
  arrange(desc(estimate)) |>
  knitr::kable(digits = 4, caption = "Estimated unsolved proportions by city")
```

| city_state         | estimate | conf.low | conf.high |
|:-------------------|---------:|---------:|----------:|
| Chicago, IL        |   0.7359 |   0.7240 |    0.7474 |
| New Orleans, LA    |   0.6485 |   0.6231 |    0.6732 |
| Baltimore, MD      |   0.6456 |   0.6276 |    0.6632 |
| San Bernardino, CA |   0.6182 |   0.5577 |    0.6753 |
| Buffalo, NY        |   0.6123 |   0.5688 |    0.6541 |
| Miami, FL          |   0.6048 |   0.5686 |    0.6400 |
| Stockton, CA       |   0.5991 |   0.5517 |    0.6447 |
| Detroit, MI        |   0.5883 |   0.5688 |    0.6076 |
| Phoenix, AZ        |   0.5514 |   0.5185 |    0.5839 |
| Denver, CO         |   0.5417 |   0.4846 |    0.5977 |
| St.¬†Louis, MO      |   0.5397 |   0.5154 |    0.5637 |
| Oakland, CA        |   0.5364 |   0.5041 |    0.5685 |
| Pittsburgh, PA     |   0.5341 |   0.4943 |    0.5735 |
| Columbus, OH       |   0.5304 |   0.5002 |    0.5605 |
| Jacksonville, FL   |   0.5111 |   0.4820 |    0.5401 |
| Minneapolis, MN    |   0.5109 |   0.4585 |    0.5631 |
| Houston, TX        |   0.5075 |   0.4892 |    0.5257 |
| San Francisco, CA  |   0.5068 |   0.4681 |    0.5454 |
| Boston, MA         |   0.5049 |   0.4646 |    0.5451 |
| Los Angeles, CA    |   0.4900 |   0.4692 |    0.5109 |
| Oklahoma City, OK  |   0.4851 |   0.4468 |    0.5236 |
| Dallas, TX         |   0.4812 |   0.4562 |    0.5062 |
| Savannah, GA       |   0.4675 |   0.4041 |    0.5319 |
| Fort Worth, TX     |   0.4645 |   0.4223 |    0.5072 |
| Baton Rouge, LA    |   0.4623 |   0.4142 |    0.5110 |
| Tampa, FL          |   0.4567 |   0.3881 |    0.5270 |
| Louisville, KY     |   0.4531 |   0.4121 |    0.4948 |
| Indianapolis, IN   |   0.4493 |   0.4223 |    0.4766 |
| Philadelphia, PA   |   0.4478 |   0.4300 |    0.4657 |
| Cincinnati, OH     |   0.4452 |   0.4080 |    0.4831 |
| Washington, DC     |   0.4379 |   0.4112 |    0.4649 |
| Birmingham, AL     |   0.4338 |   0.3992 |    0.4690 |
| San Antonio, TX    |   0.4286 |   0.3948 |    0.4630 |
| Las Vegas, NV      |   0.4142 |   0.3881 |    0.4407 |
| Omaha, NE          |   0.4132 |   0.3653 |    0.4627 |
| Long Beach, CA     |   0.4127 |   0.3629 |    0.4643 |
| Kansas City, MO    |   0.4084 |   0.3804 |    0.4370 |
| New York, NY       |   0.3876 |   0.3494 |    0.4271 |
| Albuquerque, NM    |   0.3862 |   0.3373 |    0.4376 |
| Atlanta, GA        |   0.3834 |   0.3528 |    0.4148 |
| San Diego, CA      |   0.3796 |   0.3354 |    0.4258 |
| Sacramento, CA     |   0.3697 |   0.3212 |    0.4209 |
| Durham, NC         |   0.3659 |   0.3096 |    0.4261 |
| Nashville, TN      |   0.3625 |   0.3286 |    0.3977 |
| Milwaukee, wI      |   0.3614 |   0.3333 |    0.3905 |
| Fresno, CA         |   0.3470 |   0.3051 |    0.3914 |
| Tulsa, OK          |   0.3310 |   0.2932 |    0.3711 |
| Memphis, TN        |   0.3190 |   0.2957 |    0.3433 |
| Charlotte, NC      |   0.2999 |   0.2661 |    0.3359 |
| Richmond, VA       |   0.2634 |   0.2229 |    0.3083 |
| Tulsa, AL          |   0.0000 |   0.0000 |    0.9454 |

Estimated unsolved proportions by city

## Plot

``` r
plot_city_ci <- ggplot(
  data = city_props,
  aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  geom_point(size = 2) +
  coord_flip() +
  labs(
    title = "Estimated proportion of unsolved homicides by city (95% CI)",
    x = NULL,
    y = "Proportion unsolved"
  ) +
  theme_minimal(base_size = 12)

plot_city_ci
```

<img src="p8105_hw5_xm2356_files/figure-gfm/unnamed-chunk-11-1.png" style="display: block; margin: auto;" />
